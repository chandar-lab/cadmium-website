<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <header class="paper-header">
            <h1 class="paper-title">CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design</h1>
            
            <div class="authors-section">
                <div class="authors">
                    <span class="author"><strong><a href="https://scholar.google.com/citations?user=r3XLLrEAAAAJ&hl=it&oi=ao" target="_blank">Prashant Govindarajan</a></strong><sup>*,1,2,3</sup></span>
                    <span class="author"><strong><a href="https://scholar.google.com/citations?user=PTOykNQAAAAJ&hl=it&oi=ao" target="_blank">Davide Baldelli</a></strong><sup>*,1,2,3</sup></span>
                    <span class="author"><strong><a href="https://scholar.google.com/citations?user=WKsh_bMAAAAJ&hl=it&oi=ao" target="_blank">Jay Pathak</a></strong><sup>4</sup></span>
                    <span class="author"><strong><a href="https://scholar.google.ca/citations?user=I0M6-KsAAAAJ&hl=en" target="_blank">Quentin Fournier</a></strong><sup>2</sup></span>
                    <span class="author"><strong><a href="https://scholar.google.co.in/citations?user=yxWtZLAAAAAJ&hl=en" target="_blank">Sarath Chandar</a></strong><sup>1,2,3</sup></span>
                </div>
                
                <div class="affiliations">
                    <div class="affiliation"><sup>1</sup>Chandar Research Lab</div>
                    <div class="affiliation"><sup>2</sup>Mila – Quebec AI Institute</div>
                    <div class="affiliation"><sup>3</sup>Polytechnique Montréal</div>
                    <div class="affiliation"><sup>4</sup>Ansys</div>
                </div>
                
                <div class="contact">
                    <div class="email">{firstname.lastname}@mila.quebec</div>
                    <div class="email">jay.pathak@ansys.com</div>
                </div>

                <div class="footnote">
                    <sup>*</sup>Equal contribution
                </div>
        </header>

        <!-- Links Section -->
        <div class="links-section">
            <a href="#" class="link-button">
                <img src="assets/ArxivIcon.png" alt="Paper" class="button-icon">
                <span>Paper (Coming Soon)</span>
            </a>
            <a href="https://github.com/chandar-lab/CADmium" class="link-button" target="_blank">
                <img src="assets/GithubIcon.png" alt="Code" class="button-icon">
                <span>Code</span>
            </a>
            <a href="https://huggingface.co/collections/chandar-lab/cadmium-6866b402be81f39321af98d4" class="link-button" target="_blank">
                <img src="assets/HFIcon.png" alt="Hugging Face" class="button-icon">
                <span>Models & Datasets</span>
            </a>
        </div>

        <!-- Abstract Section -->
        <section class="abstract-section">
            <!-- <h2>Abstract</h2> -->
            <p>Computer-aided design (CAD) is the digital construction of 2D and 3D objects, and is central to a wide range of engineering and manufacturing applications like automobile and aviation. Despite its importance, CAD modeling remains largely a time-intensive, manual task. Recent works have attempted to automate this process with small transformer-based models and handcrafted CAD sequence representations. However, there has been little effort to leverage the potential of large language models (LLMs) for sequential CAD design. </p>

            <h2 style="margin-top: 50px;">Contributions</h2>
            <!-- Lits of contributions -->
            <p>To address these limitations, we introduce <span class="cadmium-text">CADmium</span>, a novel approach that reformulates CAD generation as a purely <em>text-to-text</em> task. Our contributions are as follows:</p>

            <ul>
                <li>
                    We developed a <b>novel annotation pipeline</b> using <code>GPT-4.1</code> that processes multi-view images of 3D objects and their construction sequences to generate annotations combining natural language fluency with geometric precision. We applied this pipeline to 176,017 CAD sequences to release a new dataset with improved, expert-quality annotations.
                </li>
                <li>
                    We <b>fine-tuned a state-of-the-art, instruction-tuned code LLM</b>, specifically <code>Qwen-2.5-Coder</code>, on our dataset to generate JSON-formatted CAD sequences from natural language prompts. This leverages the pre-trained model's capabilities without specialized embedding layers. We evaluated our approach against leading models using established metrics and validated it on the Fusion360 Reconstruction dataset and the expert-annotated <i>CADPrompt</i> benchmark.
                </li>
                <li>
                    We introduced novel metrics (<b>sphericity, mean curvature, Euler characteristic, and watertightness</b>) to better evaluate the structural and topological characteristics of generated objects. Our work highlights the human-like fluency of CADmium's annotations, and our fine-tuned LLM achieves competitive performance, with improvements as model size increases.
                </li>
            </ul>
        </section>

        <!-- Figures Section -->
        <section class="figures-section">
            <div class="figure">
                <h2 class="figure-title">The CADmium Pipeline</h2>
                <img src="assets/MainFigure.pdf" alt="CADmium Pipeline" class="figure-image">
                <div class="figure-caption">
                    CADmium reformulates CAD generation as a purely text-to-text task. First, GPT-4.1 generates natural-sounding yet geometrically precise descriptions of 176,017 objects using their construction sequences in minimal JSON, and up to 10 multi-view images rendered with Blender. Then, the Qwen2.5-Coder LLM is fine-tuned with LoRA to translate these descriptions back into CAD sequences.
                </div>
            </div>

            <div class="figure">
                <h2 class="figure-title">CADmium vs. Text2CAD annotations</h2>
                <img src="assets/AnnotationStatistics.pdf" alt="Annotation Statistics" class="figure-image" style="width: 90%;">
                <div class="figure-caption">
                    Comparative analysis of CADmium and Text2CAD expert-level annotations. <strong>(a)</strong> Vocabulary growth as a function of token count demonstrates that Text2CAD has a limited vocabulary compared to CADmium. <strong>(b)</strong> Human-likeness, clarity, visual faithfulness given image renders, and completeness against the minimal JSON as measured by Gemma-3 12B indicates that CADmium descriptions tend to produce more natural-sounding albeit challenging descriptions. <strong>(c-e)</strong> Distribution of word counts, unique words, and digit lengths within numerical expressions per annotation shows that CADmium descriptions are more concise and diverse.
                </div>
            </div>

            <div class="figure">
                <h2 class="figure-title">Performance on CADPrompt</h2>
                <img src="assets/CadPromptTable.png" alt="CAD Prompt Results" class="figure-image" style="max-width: 60%;">
                <div class="figure-caption">
                    Performance comparison on the human-annotated <em>CADPrompt</em> dataset. Metrics are calculated on the set of valid meshes generated by each model individually.
                </div>
            </div>

            <div class="figure">
                <h2 class="figure-title">Annotation Prompt</h2>
                <img src="assets/AnnotationPrompt.pdf" alt="Annotation Prompt" class="figure-image" style="width: 80%;">
                <div class="figure-caption">
                    The user message contains the JSON description and the 10 multi-view renders of the 3D models, and instruct GPT-4.1 to generate a natural language description.
                </div>
            </div>

            <div class="figure">
                <h2 class="figure-title">Generation Prompt</h2>
                <img src="assets/GenerationPrompt.pdf" alt="Generation Prompt" class="figure-image" style="width: 80%;">
                <div class="figure-caption">
                    The system message specifies the JSON schema and constraints, while the user message contains the natural language description of the target CAD model.
                </div>
            </div>
        </section>

        <!-- Logos Section -->
        <section class="logos-section">
            <div class="logos-container">
                <img src="assets/ChandarLabLogo.png" alt="Chandar Research Lab" class="logo-image">
                <img src="assets/MilaLogo.png" alt="Mila – Quebec AI Institute" class="logo-image">
                <img src="assets/AnsysLogo.png" alt="Ansys" class="logo-image">
            </div>
        </section>

    </div>
</body>
</html>